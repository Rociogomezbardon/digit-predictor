{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I use syntehtic_digits applicatoin to create synthetic digits and labels and save them in the given files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from train import train, model\n",
    "import synthetic_digits\n",
    "\n",
    "imsize = model.IMSIZE\n",
    "images_file = 'data/DIGITS/digits.data'\n",
    "labels_file = 'data/DIGITS/labels.data'\n",
    "synthetic_digits.create_images_and_labels(images_file, labels_file, imsize )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__call__', <function LevelMapper.__call__ at 0x7f4f4966a200>), ('__init__', <function LevelMapper.__init__ at 0x7f4f4966a170>)]\n",
      "[('__call__', <function BalancedPositiveNegativeSampler.__call__ at 0x7f4f495b83b0>), ('__init__', <function BalancedPositiveNegativeSampler.__init__ at 0x7f4f495b8320>)]\n",
      "[('__init__', <function BoxCoder.__init__ at 0x7f4f495beb90>), ('decode', <function BoxCoder.decode at 0x7f4f495bed40>), ('decode_single', <function BoxCoder.decode_single at 0x7f4f495bedd0>), ('encode', <function BoxCoder.encode at 0x7f4f495bec20>), ('encode_single', <function BoxCoder.encode_single at 0x7f4f495becb0>)]\n",
      "[('__call__', <function Matcher.__call__ at 0x7f4f495be950>), ('__init__', <function Matcher.__init__ at 0x7f4f495beef0>), ('set_low_quality_matches_', <function Matcher.set_low_quality_matches_ at 0x7f4f495beb00>)]\n",
      "[('__init__', <function ImageList.__init__ at 0x7f4f495be9e0>), ('to', <function ImageList.to at 0x7f4f495bef80>)]\n",
      "[('__init__', <function Timebase.__init__ at 0x7f4f4932fa70>)]\n",
      "[('__init__', <function VideoMetaData.__init__ at 0x7f4f4932fc20>)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 45\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from collections import Counter\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.digits = np.loadtxt(images_file,dtype=float)\n",
    "        self.labels = np.loadtxt(labels_file).astype(int)-1\n",
    "        self.digits = np.reshape(self.digits,(-1,imsize,imsize))\n",
    "        self.digits = np.array([np.expand_dims(d,2) for d in self.digits])\n",
    "        c = Counter(self.labels) \n",
    "        print(c)\n",
    "    def __len__(self):\n",
    "        return len(self.digits)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.digits[idx]\n",
    "        if self.transform:\n",
    "            digit = self.transform(np.uint8(digit))\n",
    "            digit = self.transform(digit)\n",
    "        else:\n",
    "            transform = transforms.ToTensor()\n",
    "            digit = transform(digit)\n",
    "        return digit, self.labels[idx]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 103, 1: 103, 2: 103, 3: 103, 4: 103, 5: 103, 6: 103, 7: 103, 8: 103})\n"
     ]
    }
   ],
   "source": [
    "train_transform  = transforms.Compose([transforms.ToPILImage(),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.8, interpolation=3),\n",
    "        transforms.ToTensor()])\n",
    "train_dataset = myDataset(transform=train_transform)\n",
    "train_dataset.classes = ['1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "        num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'show' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0d097c41e0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'show' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "dataiter = iter(train_loader)\n",
    "#images, labels = dataiter.next()\n",
    "#images = images.numpy()\n",
    "print(len(train_loader))\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for i in range(21):\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.numpy()\n",
    "    for idx in np.arange(45):\n",
    "        ax = fig.add_subplot(3, 15, idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "        ax.set_title(str(labels[idx].item()+1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNet(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=9, bias=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model1 = model.CNNet()\n",
    "#model1.load_state_dict(torch.load('best_model.pth'))\n",
    "model1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, CELoss: 2.197383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ec91099dab30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"best_model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/git/sudokusolver/digit_predictor/train/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, epochs, device, learning_rate, momentum, best_model_path, first_epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/anaconda3/envs/computer-vision/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/anaconda3/envs/computer-vision/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    121\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    122\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train.train(model1, train_loader, epochs, device, learning_rate, momentum,  \"best_model.pth\", first_epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform  = transforms.Compose([transforms.ToPILImage(),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=1, interpolation=3),\n",
    "        transforms.ToTensor()])\n",
    "test_dataset = myDataset(transform=train_transform)\n",
    "test_dataset.classes = ['1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(45):\n",
    "    ax = fig.add_subplot(3, 15, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(str(labels[idx].item()+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(9))\n",
    "class_total = list(0. for i in range(9))\n",
    "\n",
    "model1.to(device)\n",
    "model1.eval() # prep model for *evaluation*\n",
    "\n",
    "for batch in train_loader:\n",
    "    batch_X, batch_y = batch\n",
    "\n",
    "    batch_X = batch_X.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "\n",
    "    model1.zero_grad()\n",
    "    output = model1(batch_X)\n",
    "    loss = criterion(output, batch_y) \n",
    "    \n",
    "    test_loss += loss.item()*batch_X.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct = np.squeeze(pred.eq(batch_y.data.view_as(pred)))\n",
    "\n",
    "    for i in range(len(batch_y)):\n",
    "        label = batch_y.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(9):\n",
    "    print(i)\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i+1), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (test_dataset.classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
